# ğŸ¦™ LLaMA Text Summarizer

A simple AI-powered web application that summarizes text using the **LLaMA** model (via [Ollama](https://ollama.com)), with a **FastAPI** backend and **Streamlit** frontend.  

---

## âœ¨ Features
- âš¡ **FastAPI Backend** â€“ Handles text summarization requests and communicates with the LLaMA model.  
- ğŸ¨ **Streamlit Frontend** â€“ User-friendly web interface for entering text and viewing summaries.  
- ğŸ’» **Runs Locally** â€“ Uses Ollama to host the LLaMA model on your machine.  
- ğŸ”— **Git & GitHub** â€“ Version control and project collaboration ready.  

---

## ğŸ“‚ Project Structure
```
text-summarizer-llama/
â”‚
â”œâ”€â”€ backend/
â”‚   â””â”€â”€ main.py           # FastAPI backend
â”‚
â”œâ”€â”€ frontend/
â”‚   â””â”€â”€ app.py            # Streamlit frontend
â”‚
â”œâ”€â”€ venv/                 # Virtual environment (excluded in .gitignore)
â”œâ”€â”€ requirements.txt      # Python dependencies
â””â”€â”€ README.md             # Documentation
```

---

## ğŸš€ Installation & Setup

### **1. Install Python**
- Download & install Python from [python.org/downloads](https://www.python.org/downloads/).
- Ensure you check **"Add Python to PATH"** during installation.

### **2. Clone the Repository**
```bash
git clone https://github.com/iamdevlab/llama-text-summarizer.git
cd llama-text-summarizer
```

### **3. Create & Activate Virtual Environment**
**Windows (PowerShell)**:
```powershell
python -m venv venv
venv\Scripts\Activate.ps1
```
**Mac/Linux**:
```bash
python3 -m venv venv
source venv/bin/activate
```

### **4. Install Dependencies**
```bash
pip install -r requirements.txt
```

### **5. Install Ollama & LLaMA Model**
- Download Ollama from [ollama.com/download](https://ollama.com/download).  
- Pull the LLaMA model:
```bash
ollama pull llama2
```

---

## â–¶ï¸ Running the App

### **Run Backend (FastAPI)**
```bash
uvicorn backend.main:app --reload
```
Backend runs at: **http://localhost:8000**

### **Run Frontend (Streamlit)**
In a new terminal:
```bash
streamlit run frontend/app.py
```
Frontend runs at: **http://localhost:8501**

---

## ğŸ“Š Workflow Diagram
![Workflow Diagram](docs/workflow.png)  
*(User â†’ Streamlit â†’ FastAPI â†’ Ollama (LLaMA) â†’ Summary Output)*

---

## ğŸ›  Tech Stack
- **[Python](https://www.python.org/)** â€“ Programming language
- **[FastAPI](https://fastapi.tiangolo.com/)** â€“ Backend API
- **[Streamlit](https://streamlit.io/)** â€“ Frontend UI
- **[Ollama](https://ollama.com/)** â€“ Local LLaMA model hosting
- **[Git & GitHub](https://github.com/)** â€“ Version control

---

## ğŸ“œ License
This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ’¡ Author
Developed by **[Vernon Usigbe](https://github.com/iamdevlab)**.  
If you find this useful, â­ the repo and share it!
